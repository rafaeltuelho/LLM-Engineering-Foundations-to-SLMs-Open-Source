{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZwKP3f2HGz9"
      },
      "source": [
        "# Encoder-Decoder Transformer Model from Scratch in PyTorch\n",
        "\n",
        "In today's notebook, we'll be focusing on two major components of transformers:\n",
        "\n",
        "1. The Building Blocks\n",
        "2. Training the Model\n",
        "\n",
        "As a brunt of the in-class time will be spent on the building blocks, we'll leave the training logic as an assignment to complete. We'll be focusing more specifically on training once we start using decoder-only architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjbGDn9SF49m"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "Throughout our time together - we'll be providing a number of assignments. Each assignment will be split into two broad categories:\n",
        "\n",
        "1. Base Assignment - a more conceptual and theory based assignment focused on locking in specific key concepts and learnings.\n",
        "2. Hardmode Assignment - a more programming focused assignment focused on core code-concepts used in transformers.\n",
        "\n",
        "Each assignment will have a few of the following categories of exercises:\n",
        "\n",
        "1. ❓Questions - these will be questions that you will be expected to gather the answer to!\n",
        "2. 🏗️ Activities - these will be work or coding activities meant to reinforce specific concepts or theory components.\n",
        "\n",
        "You are expected to complete all of the activities in your selected notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z43kfJGvlQyA"
      },
      "source": [
        "# The Building Block Fundamentals of Transformer Architecture\n",
        "\n",
        "We're going to start with an example of an encoder-decoder model - the kind found in the classic paper:\n",
        "\n",
        "[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "We'll walk through each step in code - leveraging the [PyTorch]() library heavily - in order to get an idea of how these models work.\n",
        "\n",
        "While this example notebook could be extended to a sincere usecase - we'll be using a toy dataset, and we will not fully train the model until it converges (under-train), as the full training process might take many days!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sd2KYudl0Hn"
      },
      "source": [
        "## The Desired Architecture\n",
        "\n",
        "![image](https://i.imgur.com/YPjbqW6.png)\n",
        "\n",
        "We'll skip over the diagram for now, and talk through each component in detail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TPUnyvsuovuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8M3oT0l8fM"
      },
      "source": [
        "## Embedding\n",
        "\n",
        "![image](https://i.imgur.com/sFlEZ2e.png)\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ddpQjPJWmXZg"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int, verbose=False) -> None:\n",
        "    \"\"\"\n",
        "    vocab_size - the size of our vocabulary\n",
        "    d_model - the dimension of our embeddings and the input dimension for our model\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.verbose:\n",
        "      print(f\"Embedding Vector (1st 5 elements): {self.embedding(x)[:5] * math.sqrt(self.d_model)}\")\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # scale embeddings by square root of d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Q4WPNALUKR"
      },
      "source": [
        "### ❓Question 1:\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Batch Size = `16`\n",
        "2. Sequence Length = `350`\n",
        "\n",
        "What will the output shape of the `InputEmbeddings` layer be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8149a5d",
        "outputId": "36a67aba-b3fe-406b-df51-8e070b52dadf"
      },
      "source": [
        "import torch\n",
        "\n",
        "batch_size = 13\n",
        "seq_len = 350\n",
        "d_model = 512\n",
        "\n",
        "dummy_tensor = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(f\"Shape of the dummy tensor: {dummy_tensor.shape}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dummy tensor: torch.Size([13, 350, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCx8DYkaxwto"
      },
      "source": [
        "### Test Embedding Layer\n",
        "\n",
        "We'll set up a sample Embedding Layer and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xl9VdXuYwfVU"
      },
      "outputs": [],
      "source": [
        "def test_input_embeddings_with_example():\n",
        "    # Create a small embedding layer\n",
        "    embed = InputEmbeddings(d_model=512, vocab_size=1000)\n",
        "\n",
        "    # Example sentence tokens (simplified)\n",
        "    tokens = torch.tensor([[1, 2, 3, 4, 5]])  # \"The cat sat down quickly\"\n",
        "\n",
        "    output = embed(tokens)\n",
        "    print(f\"Input shape: {tokens.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"\\nExample shows how words are converted to high-dimensional vectors\")\n",
        "\n",
        "    # Run technical test\n",
        "    assert output.shape == (1, 5, 512), f\"Expected shape (1, 5, 512), got {output.shape}\"\n",
        "    print(\"✓ Input Embeddings Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBIElPiqxORa",
        "outputId": "477638d1-3ff3-42b6-b557-04f970072c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([1, 5])\n",
            "Output shape: torch.Size([1, 5, 512])\n",
            "\n",
            "Example shows how words are converted to high-dimensional vectors\n",
            "✓ Input Embeddings Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_input_embeddings_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5KCa1KnfrC"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "![image](https://i.imgur.com/IIA3NK3.png)\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We're going to use the process outlined in the paper to do this - which is to use a specific combination of functions to add positional information to the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5K3NXh7MoM5D"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=True) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(f\"Embedding Encodings before positional addition (1st 5 elements): {x}\")\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x}\")\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKGHvs0eMCoQ"
      },
      "source": [
        "### ❓Question 2:\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Batch Size = `16`\n",
        "2. Sequence Length = `350`\n",
        "\n",
        "What will the output shape of the `PositionalEncoding` layer be?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape is `(16, 350, 512)`."
      ],
      "metadata": {
        "id": "XBe6MZFEfg7C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkcMmoT2yOZs"
      },
      "source": [
        "### Test Positional Encoding Layer\n",
        "\n",
        "We'll set up a sample Positional Encoding Layer and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ayoWMbSYySBp"
      },
      "outputs": [],
      "source": [
        "def test_positional_encoding_with_example():\n",
        "    pos = PositionalEncoding(d_model=512, seq_len=350, dropout=0.1)\n",
        "\n",
        "    # Create sample embeddings for \"The cat sat\"\n",
        "    x = torch.randn(16, 350, 512)\n",
        "\n",
        "    output = pos(x)\n",
        "    print(\"Input tokens position:  [1, 2, 3]\")\n",
        "    print(\"Added position info to each word's embedding\")\n",
        "    print(f\"Output maintains shape: {output.shape}\")\n",
        "\n",
        "    # Verify position information was added\n",
        "    assert not torch.allclose(output, x), \"Position information should modify embeddings\"\n",
        "    print(\"✓ Positional Encoding Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4hDXJaGyU5M",
        "outputId": "f868a6f5-88dc-4f0d-f7cc-c698873e5cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Encodings before positional addition (1st 5 elements): tensor([[[ 0.0785, -0.5779, -0.4519,  ...,  0.7634,  0.7781, -2.0705],\n",
            "         [-0.7535,  1.2859, -0.7521,  ...,  0.4039, -0.1534,  1.4387],\n",
            "         [-0.3176, -1.0120,  1.4094,  ..., -0.5970,  0.2767,  0.3505],\n",
            "         ...,\n",
            "         [-1.8538,  0.4306,  0.4291,  ...,  0.3845,  1.4026,  0.4167],\n",
            "         [ 0.4182,  1.1831, -1.2655,  ..., -1.7682,  0.9877,  1.3731],\n",
            "         [-1.2397, -1.4224,  0.5417,  ...,  1.7713, -0.3998,  1.3644]],\n",
            "\n",
            "        [[-0.2218, -0.0239,  0.3838,  ..., -1.3235,  0.3070, -0.9520],\n",
            "         [-0.6849,  0.1534, -2.4834,  ..., -0.0975, -1.4550,  0.2529],\n",
            "         [-0.0960, -0.4984, -0.6242,  ...,  1.3293,  0.5674,  0.7876],\n",
            "         ...,\n",
            "         [-0.2043,  0.9094, -0.4458,  ...,  1.6735, -1.4790,  0.8489],\n",
            "         [-2.0662, -1.4072,  0.7063,  ..., -0.4247, -0.0619,  0.8813],\n",
            "         [-0.6963, -1.0385, -0.5797,  ..., -1.5279,  0.1239, -0.1357]],\n",
            "\n",
            "        [[ 0.4353, -1.7351,  0.9989,  ...,  1.6740, -1.0415,  0.0630],\n",
            "         [ 0.0332, -1.0463,  1.0575,  ...,  1.8824,  1.7483, -0.3879],\n",
            "         [-1.9198,  1.1152,  1.2223,  ...,  0.5042, -0.2107, -0.0955],\n",
            "         ...,\n",
            "         [-1.5494, -0.0643,  0.1576,  ...,  0.9230,  2.5682,  1.5314],\n",
            "         [-0.7493, -1.3005, -0.5557,  ...,  0.9149,  1.1372, -0.2471],\n",
            "         [ 0.3342, -1.2667,  1.4281,  ...,  2.5939, -0.9658, -0.8300]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.0271, -2.6738,  0.3347,  ..., -1.0786,  1.1197, -0.8291],\n",
            "         [ 1.2421,  3.0586, -0.1667,  ...,  1.4277, -0.9542, -1.6544],\n",
            "         [ 0.8537, -0.6245, -0.5334,  ...,  0.6274, -0.1542,  0.7786],\n",
            "         ...,\n",
            "         [ 2.1110,  0.8159,  0.5872,  ..., -2.3959,  1.4983, -0.5377],\n",
            "         [-0.7082, -1.3903,  0.5967,  ..., -0.1111,  1.9085,  0.2987],\n",
            "         [ 1.3053,  0.7601,  1.6631,  ...,  0.7043,  0.2046, -0.9609]],\n",
            "\n",
            "        [[ 1.0935, -0.5589, -0.2042,  ..., -0.8272,  0.7334, -0.1315],\n",
            "         [ 0.9987,  0.2334,  0.1698,  ...,  1.1866,  1.2811, -1.5231],\n",
            "         [ 0.5991, -1.1115,  0.0344,  ...,  0.5022,  0.8687, -0.8559],\n",
            "         ...,\n",
            "         [-0.3449,  0.7396, -1.0240,  ...,  0.5116,  0.2734,  1.7386],\n",
            "         [-1.6161,  0.2413,  0.8347,  ...,  0.3633,  0.1495, -0.9048],\n",
            "         [ 0.1115, -0.1534, -0.1520,  ..., -0.1805, -0.3486, -0.4914]],\n",
            "\n",
            "        [[-1.3849, -0.6265, -0.5312,  ...,  2.9927, -0.1814, -0.0266],\n",
            "         [-0.8540, -0.0247, -0.0965,  ..., -1.0339,  1.3335,  0.6201],\n",
            "         [ 2.2389, -2.8443, -0.3254,  ..., -2.4512, -0.1304, -1.9406],\n",
            "         ...,\n",
            "         [-0.9948, -1.9667, -1.3149,  ...,  0.5540,  0.1243, -0.1072],\n",
            "         [ 1.2908,  0.4669,  0.4373,  ..., -2.0986,  1.6012, -0.6625],\n",
            "         [-0.0063,  0.7579, -1.5832,  ...,  1.8151, -1.5611,  1.0145]]])\n",
            "Positional Encodings (1st 5 elements): tensor([[[ 0.0785,  0.4221, -0.4519,  ...,  1.7634,  0.7781, -1.0705],\n",
            "         [ 0.0880,  1.8262,  0.0697,  ...,  1.4039, -0.1533,  2.4387],\n",
            "         [ 0.5917, -1.4282,  2.3458,  ...,  0.4030,  0.2769,  1.3505],\n",
            "         ...,\n",
            "         [-0.8644,  0.5761,  1.4167,  ...,  1.3838,  1.4386,  1.4161],\n",
            "         [ 1.0752,  0.4292, -0.8322,  ..., -0.7689,  1.0238,  2.3724],\n",
            "         [-1.5191, -2.3826,  0.0479,  ...,  2.7706, -0.3636,  2.3637]],\n",
            "\n",
            "        [[-0.2218,  0.9761,  0.3838,  ..., -0.3235,  0.3070,  0.0480],\n",
            "         [ 0.1566,  0.6937, -1.6616,  ...,  0.9025, -1.4549,  1.2529],\n",
            "         [ 0.8133, -0.9145,  0.3123,  ...,  2.3293,  0.5676,  1.7876],\n",
            "         ...,\n",
            "         [ 0.7851,  1.0549,  0.5417,  ...,  2.6728, -1.4430,  1.8483],\n",
            "         [-1.4093, -2.1611,  1.1396,  ...,  0.5746, -0.0259,  1.8807],\n",
            "         [-0.9757, -1.9986, -1.0735,  ..., -0.5286,  0.1601,  0.8637]],\n",
            "\n",
            "        [[ 0.4353, -0.7351,  0.9989,  ...,  2.6740, -1.0415,  1.0630],\n",
            "         [ 0.8747, -0.5060,  1.8793,  ...,  2.8824,  1.7484,  0.6121],\n",
            "         [-1.0105,  0.6991,  2.1587,  ...,  1.5042, -0.2105,  0.9045],\n",
            "         ...,\n",
            "         [-0.5600,  0.0811,  1.1452,  ...,  1.9223,  2.6042,  2.5308],\n",
            "         [-0.0923, -2.0545, -0.1223,  ...,  1.9142,  1.1733,  0.7523],\n",
            "         [ 0.0548, -2.2269,  0.9343,  ...,  3.5932, -0.9296,  0.1693]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.0271, -1.6738,  0.3347,  ..., -0.0786,  1.1197,  0.1709],\n",
            "         [ 2.0836,  3.5989,  0.6552,  ...,  2.4277, -0.9541, -0.6544],\n",
            "         [ 1.7630, -1.0407,  0.4030,  ...,  1.6274, -0.1540,  1.7786],\n",
            "         ...,\n",
            "         [ 3.1003,  0.9613,  1.5748,  ..., -1.3965,  1.5342,  0.4617],\n",
            "         [-0.0512, -2.1442,  1.0300,  ...,  0.8882,  1.9445,  1.2980],\n",
            "         [ 1.0258, -0.2001,  1.1692,  ...,  1.7036,  0.2408,  0.0384]],\n",
            "\n",
            "        [[ 1.0935,  0.4411, -0.2042,  ...,  0.1728,  0.7334,  0.8685],\n",
            "         [ 1.8402,  0.7737,  0.9916,  ...,  2.1866,  1.2812, -0.5231],\n",
            "         [ 1.5084, -1.5276,  0.9708,  ...,  1.5022,  0.8689,  0.1441],\n",
            "         ...,\n",
            "         [ 0.6445,  0.8850, -0.0365,  ...,  1.5109,  0.3094,  2.7379],\n",
            "         [-0.9591, -0.5126,  1.2680,  ...,  1.3626,  0.1856,  0.0946],\n",
            "         [-0.1679, -1.1136, -0.6458,  ...,  0.8187, -0.3124,  0.5079]],\n",
            "\n",
            "        [[-1.3849,  0.3735, -0.5312,  ...,  3.9927, -0.1814,  0.9734],\n",
            "         [-0.0125,  0.5156,  0.7254,  ..., -0.0339,  1.3336,  1.6201],\n",
            "         [ 3.1482, -3.2605,  0.6110,  ..., -1.4512, -0.1302, -0.9406],\n",
            "         ...,\n",
            "         [-0.0054, -1.8213, -0.3273,  ...,  1.5533,  0.1603,  0.8922],\n",
            "         [ 1.9478, -0.2871,  0.8706,  ..., -1.0993,  1.6373,  0.3369],\n",
            "         [-0.2857, -0.2023, -2.0770,  ...,  2.8144, -1.5249,  2.0138]]])\n",
            "Input tokens position:  [1, 2, 3]\n",
            "Added position info to each word's embedding\n",
            "Output maintains shape: torch.Size([16, 350, 512])\n",
            "✓ Positional Encoding Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_positional_encoding_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueiM7LzKpcFY"
      },
      "source": [
        "## Add & Norm\n",
        "\n",
        "Next we'll tackle the Add & Norm Block of the diagram.\n",
        "\n",
        "![image](https://i.imgur.com/otdEq4D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDABPLSKqOt"
      },
      "source": [
        "### Layer Normalization\n",
        "\n",
        "The first step is to add layer normalization. You can read more about it [here](https://paperswithcode.com/method/layer-normalization)!\n",
        "\n",
        "The basic idea is that it makes training the model a bit easier, and allows the model to generalize a bit better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1Nlv7BH7ruSf"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, features: int, epsilon:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim = -1, keepdim = True)\n",
        "    standard_deviation = x.std(dim = -1, keepdim = True)\n",
        "    return self.gamma * (x - mean) / (standard_deviation + self.epsilon) + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg-deMc3MNnM"
      },
      "source": [
        "### ❓Question 3:\n",
        "\n",
        "What is the purpose of `epsilon` in the above code.\n",
        "\n",
        "> HINT: Pay special attention to the math in the `return` statement."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its purpose is to prevent division by zero in cases where the standard deviation of the input is extremely small or zero. This ensures numerical stability during training."
      ],
      "metadata": {
        "id": "W8bFYg2ghoN1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vft5kFUBy1aE"
      },
      "source": [
        "### Test Layer Normalization\n",
        "\n",
        "We'll set up a sample Layer Normalization and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "01Llr9cwy7C2"
      },
      "outputs": [],
      "source": [
        "def test_layer_normalization_with_example():\n",
        "    layer_norm = LayerNormalization(features=3)  # Smaller feature size for example\n",
        "\n",
        "    # Simulate word embeddings with different magnitudes\n",
        "    word_embeddings = torch.tensor([\n",
        "        [2.5, 4.1, -3.2],  # \"The\" (high magnitude)\n",
        "        [0.1, 0.2, -0.1],  # \"cat\" (low magnitude)\n",
        "        [8.2, -6.1, 5.5]   # \"sat\" (very high magnitude)\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    normalized = layer_norm(word_embeddings)\n",
        "\n",
        "    print(\"Before normalization (magnitudes vary greatly):\")\n",
        "    print(word_embeddings[0])\n",
        "    print(\"\\nAfter normalization (values scaled to similar ranges):\")\n",
        "    print(normalized[0])\n",
        "\n",
        "    # Verify statistical properties\n",
        "    mean = normalized.mean(dim=-1)\n",
        "    var = normalized.var(dim=-1)\n",
        "    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-5)\n",
        "    assert torch.allclose(var, torch.ones_like(var), atol=1e-5)\n",
        "    print(\"✓ Layer Normalization Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75hToLCry9mR",
        "outputId": "7462bfe3-e045-4277-f956-2e0720896c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before normalization (magnitudes vary greatly):\n",
            "tensor([[ 2.5000,  4.1000, -3.2000],\n",
            "        [ 0.1000,  0.2000, -0.1000],\n",
            "        [ 8.2000, -6.1000,  5.5000]])\n",
            "\n",
            "After normalization (values scaled to similar ranges):\n",
            "tensor([[ 0.3562,  0.7732, -1.1293],\n",
            "        [ 0.2182,  0.8729, -1.0911],\n",
            "        [ 0.7459, -1.1363,  0.3905]], grad_fn=<SelectBackward0>)\n",
            "✓ Layer Normalization Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_layer_normalization_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXt7KKKycrl"
      },
      "source": [
        "### Residual Connection\n",
        "\n",
        "Another technique that makes model training easier, we add a Residual connection to the outputs of the Attention Block - this helps to prevent vanishing gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XKwGFD2-yd-Z"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layernorm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.layernorm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p668YYgBzc3G"
      },
      "source": [
        "### Testing Residual Connection\n",
        "\n",
        "We'll set up a sample Residual Connection and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "heebV5mIzje4"
      },
      "outputs": [],
      "source": [
        "def test_residual_connection_with_example():\n",
        "    residual = ResidualConnection(features=3, dropout=0.1)\n",
        "\n",
        "    # Original input \"The cat\"\n",
        "    x = torch.tensor([\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [2.0, 2.0, 2.0]\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    # Sublayer that makes meaningful changes\n",
        "    def sublayer(x):\n",
        "        return torch.nn.functional.relu(x + 0.5) # Non-linear transformation\n",
        "\n",
        "    output = residual(x, sublayer)\n",
        "\n",
        "    print(\"Original input:\")\n",
        "    print(x[0])\n",
        "    print(\"\\nAfter residual connection (combines original + transformed):\")\n",
        "    print(output[0])\n",
        "\n",
        "    # Verify output changed but maintained shape\n",
        "    assert output.shape == x.shape\n",
        "    assert torch.any(torch.abs(output - x) > 1e-6), \"Output should differ from input\"\n",
        "    print(\"✓ Residual Connection Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMF65cmzmgx",
        "outputId": "35e75ea4-a990-465f-9459-3283dee383b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original input:\n",
            "tensor([[1., 1., 1.],\n",
            "        [2., 2., 2.]])\n",
            "\n",
            "After residual connection (combines original + transformed):\n",
            "tensor([[1.5556, 1.5556, 1.5556],\n",
            "        [2.5556, 2.5556, 2.5556]], grad_fn=<SelectBackward0>)\n",
            "✓ Residual Connection Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_residual_connection_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIOZp3xhsaXK"
      },
      "source": [
        "## Feed Forward Network\n",
        "\n",
        "![image](https://i.imgur.com/woEqBjQ.png)\n",
        "\n",
        "Moving onto the next component, we have our feed forward network.\n",
        "\n",
        "The feed forward networks servers two purposes in our model:\n",
        "\n",
        "1. It reforms the attention outputs into a format that works with the next block.\n",
        "\n",
        "2. It helps add complexity to prevent each attention block acting in a similar fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JueNG2UBszbR"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1) -> None:\n",
        "    \"\"\"\n",
        "    d_model - dimension of model\n",
        "    d_ff - dimension of feed forward network\n",
        "    dropout - regularization measure\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU-H1m4L0UhY"
      },
      "source": [
        "### Testing the Feed-forward Block\n",
        "\n",
        "Let's test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pwtbsJou0fxz"
      },
      "outputs": [],
      "source": [
        "def test_feed_forward_block_with_example():\n",
        "   ff_block = FeedForwardBlock(d_model=3, d_ff=8)  # Small dimensions for demonstration\n",
        "\n",
        "   # Input: Word embeddings for \"The cat\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 0.5, 0.2],  # \"The\"\n",
        "       [2.0, -0.3, 1.1]  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   output = ff_block(x)\n",
        "\n",
        "   print(\"Input embeddings:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter feed-forward transformation:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # First linear layer expands to d_ff dimensions\n",
        "   # ReLU keeps only positive values\n",
        "   # Second linear layer projects back to d_model dimensions\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Feed Forward Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMHE9fWB0hj3",
        "outputId": "16aaac17-7218-4fcb-f163-fb5177bf9bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings:\n",
            "tensor([[ 1.0000,  0.5000,  0.2000],\n",
            "        [ 2.0000, -0.3000,  1.1000]])\n",
            "\n",
            "After feed-forward transformation:\n",
            "tensor([[-0.3064,  0.0267,  0.1877],\n",
            "        [-0.3330, -0.0836,  0.3992]], grad_fn=<SelectBackward0>)\n",
            "✓ Feed Forward Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_feed_forward_block_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5EGkvdtP0O"
      },
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "![image](https://i.imgur.com/4qOT46y.png)\n",
        "\n",
        "Next up is the heart and soul of the Transformer - Multi-Head Attention.\n",
        "\n",
        "We'll break it down into the basic building blocks in code in the following section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oto52ZyvX0k"
      },
      "source": [
        "### Multi-Head Attention Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mDHkw1f_vmuv"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    query = self.w_q(query)\n",
        "    key = self.w_k(key)\n",
        "    value = self.w_v(value)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEm7I6iwMiom"
      },
      "source": [
        "### ❓Question 4:\n",
        "\n",
        "What do: Q, K, V, and O stand for in the above code?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Q stands for Query. It represents the query vector for each token in the\n",
        "sequence. When computing attention, each token's query is compared against all other tokens' keys to determine how much attention to pay to them.\n",
        " * K stands for Key. It represents the key vector for each token in the sequence. Keys are used to determine the relevance of a token to a given query.\n",
        " * V stands for Value. It represents the value vector for each token. Once the attention scores are calculated using queries and keys, these scores are used to weigh the values, effectively creating a weighted sum of the value vectors.\n",
        " * O stands for Output. This is the result of the attention mechanism after the weighted values are combined and passed through a final linear layer (w_o). This output represents the attended-to information for each token."
      ],
      "metadata": {
        "id": "aZk9kA5JipIC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dp3eM7H1cRl"
      },
      "source": [
        "### Testing Multi-Head Attention\n",
        "\n",
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "O6R_o-c-1fCu"
      },
      "outputs": [],
      "source": [
        "def test_multi_head_attention_with_example():\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)  # Small dimensions for clarity\n",
        "\n",
        "   # Input sequence: [\"The\", \"cat\", \"sat\"]\n",
        "   query = key = value = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]   # \"sat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Allow all words to attend to each other\n",
        "   mask = torch.ones(1, 1, 3, 3)\n",
        "\n",
        "   output = mha(query, key, value, mask)\n",
        "\n",
        "   print(\"Input embeddings (each row is a word):\")\n",
        "   print(query[0])\n",
        "   print(\"\\nAttention output (words now contain mixed information from relevant words):\")\n",
        "   print(output[0])\n",
        "\n",
        "   # Each head processes sequence differently, then results are combined\n",
        "   assert output.shape == query.shape\n",
        "   assert not torch.allclose(output, query)\n",
        "   print(\"✓ Multi-Head Attention Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owCjHXhi1gYL",
        "outputId": "5cb3cc64-26ee-46d5-a53e-22a88b24d190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings (each row is a word):\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "Attention output (words now contain mixed information from relevant words):\n",
            "tensor([[-0.1895, -0.1183,  0.1418,  0.0203, -0.1421,  0.0187],\n",
            "        [-0.2167, -0.1249,  0.1384,  0.0361, -0.1299, -0.0263],\n",
            "        [-0.1968, -0.1211,  0.1342,  0.0134, -0.1415,  0.0180]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Multi-Head Attention Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_multi_head_attention_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyD7nAM6tb0K"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "![image](https://i.imgur.com/Yp48DuB.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKk08SvowJIc"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask, d_k, dropout: nn.Dropout = None):\n",
        "  attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "  attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "  if dropout is not None:\n",
        "    attention_scores = dropout(attention_scores)\n",
        "\n",
        "  return (attention_scores @ value), attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac8k7b3SxKOC"
      },
      "source": [
        "### Forward Method\n",
        "\n",
        "This is code is required to do a forward pass with our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um2Oxv1axWYK"
      },
      "outputs": [],
      "source": [
        "def forward(self, query, key, value, mask):\n",
        "  query = self.w_q(query)\n",
        "  key = self.w_k(key)\n",
        "  value = self.w_v(value)\n",
        "\n",
        "  query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "  x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "  return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgr_D0mmyEgp"
      },
      "source": [
        "### Combining it All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odYKdmwMyH4P"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    query = self.w_q(query)\n",
        "    key = self.w_k(key)\n",
        "    value = self.w_v(value)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFqoZyD1-AU"
      },
      "source": [
        "### Testing MultiHeadAttention\n",
        "\n",
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sY1N60di2CHQ"
      },
      "outputs": [],
      "source": [
        "def test_attention_mechanism():\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "\n",
        "   # Simple sequence: \"The cat sleeps\"\n",
        "   seq = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
        "   ]).unsqueeze(0)  # [1, 3, 6]\n",
        "\n",
        "   # Mask shape needs to match attention scores [batch, heads, seq_len, seq_len]\n",
        "   attention_scores = torch.ones(1, 2, 3, 3)  # 2 heads, sequence length 3\n",
        "\n",
        "   print(\"Input sequence shape:\", seq.shape)\n",
        "   print(\"Input values (each row is a word):\")\n",
        "   print(seq[0])\n",
        "\n",
        "   output = mha(seq, seq, seq, attention_scores)\n",
        "   print(\"\\nOutput after attention:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # Verify output maintains shape but changes values\n",
        "   assert output.shape == seq.shape\n",
        "   assert not torch.allclose(output, seq)\n",
        "   print(\"✓ Multi-Head Attention Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qEqn4BS2DHt",
        "outputId": "85d1adef-632d-473e-faa1-5c8af8b0c84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence shape: torch.Size([1, 3, 6])\n",
            "Input values (each row is a word):\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "Output after attention:\n",
            "tensor([[ 0.0167, -0.0633,  0.0374, -0.0691,  0.0106, -0.0259],\n",
            "        [ 0.0145, -0.0651,  0.0345, -0.0696,  0.0149, -0.0238],\n",
            "        [ 0.0129, -0.0684,  0.0366, -0.0683,  0.0183, -0.0246]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Multi-Head Attention Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_attention_mechanism()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjLhpiyz5b"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "When we pass information through our model - the first thing we will do is Encode it by passing it through our Encoder Blocks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0edIfMzijj"
      },
      "source": [
        "### Encoder Block\n",
        "\n",
        "![image](https://i.imgur.com/nwNYZAT.png)\n",
        "\n",
        "The encoder takes in the source language sentence (e.g. English). Each word is converted into a vector representation using an embedding layer. Then a positional encoder adds information about the position of each word. This goes through multiple self-attention layers, where each word vector attends to all other word vectors to build contextual representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMVnZiGDy1PG"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, input_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, input_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_twqqReS28ul"
      },
      "source": [
        "### Testing the EncoderBlock\n",
        "\n",
        "Testing time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP_wq0R02_g0"
      },
      "outputs": [],
      "source": [
        "def test_encoder_block():\n",
        "   # Create encoder block with small dimensions\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   ff = FeedForwardBlock(d_model=6, d_ff=12)\n",
        "   encoder = EncoderBlock(features=6, self_attention_block=mha, feed_forward_block=ff, dropout=0.1)\n",
        "\n",
        "   # Input: \"The cat sleeps\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]   # \"sleeps\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Attention mask\n",
        "   mask = torch.ones(1, 2, 3, 3)  # Allow all connections\n",
        "\n",
        "   output = encoder(x, mask)\n",
        "\n",
        "   print(\"Input sequence:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter encoder processing (self-attention + feed-forward):\")\n",
        "   print(output[0])\n",
        "\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Encoder Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPAH-NKA3AEN",
        "outputId": "81589cd6-c882-4469-c38f-536be8c32d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "After encoder processing (self-attention + feed-forward):\n",
            "tensor([[ 1.5212,  0.8665, -0.8088,  0.6585,  0.7103, -1.0204],\n",
            "        [ 0.5185,  0.0000,  0.8880,  1.7109, -0.4607,  0.0312],\n",
            "        [-0.4288,  0.2765, -0.0639,  0.2137,  1.1175,  0.4317]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Encoder Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_encoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AvnoPrzvwu"
      },
      "source": [
        "### Encoder Stack\n",
        "\n",
        "Following along from the original paper - we will organize these blocks into a set of 6.\n",
        "\n",
        "These 6 Encoder Blocks (each with 8 Attention Heads) will comprise our Encoding Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOaR5SjUzxv7"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQyujsRTz5NT"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "Next, we will take the encoded sequence and decode it through our Decoder Blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBYgl77Kz6bx"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "![image](https://i.imgur.com/HtAAXZc.png)\n",
        "\n",
        "The decoder takes in the target language sentence (e.g. Italian). It also converts words to vectors and adds positional info. Then it goes through self-attention layers. Here, a mask is applied so each word can only see the words before it, not after.\n",
        "\n",
        "The decoder also does attention over the encoder output. This allows each French word to find relevant connections with the English words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIwafbqzz5-n"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, input_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0KkwjkM3xe7"
      },
      "source": [
        "### Testing DecoderBlock\n",
        "\n",
        "You know what's up next...testing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnmnWAVl307S"
      },
      "outputs": [],
      "source": [
        "def test_decoder_block():\n",
        "   # Initialize components with small dimensions\n",
        "   self_attn = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   cross_attn = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   ff = FeedForwardBlock(d_model=6, d_ff=12)\n",
        "   decoder = DecoderBlock(features=6, self_attention_block=self_attn,\n",
        "                         cross_attention_block=cross_attn,\n",
        "                         feed_forward_block=ff, dropout=0.1)\n",
        "\n",
        "   # Input: \"El gato\" (target sequence)\n",
        "   x = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"El\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"gato\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Encoder output: \"The cat\" (source sequence)\n",
        "   encoder_output = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Masks\n",
        "   src_mask = torch.ones(1, 2, 2, 2)  # Can attend to all encoder outputs\n",
        "   tgt_mask = torch.tril(torch.ones(1, 2, 2, 2))  # Can only attend to previous words\n",
        "\n",
        "   output = decoder(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "   print(\"Input target sequence:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nSource (encoder) sequence:\")\n",
        "   print(encoder_output[0])\n",
        "   print(\"\\nDecoder output (after self-attention, cross-attention, and feed-forward):\")\n",
        "   print(output[0])\n",
        "\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Decoder Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COJlCq3033A2",
        "outputId": "6b4ef755-228d-49c3-f8e5-fff106168a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input target sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]])\n",
            "\n",
            "Source (encoder) sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]])\n",
            "\n",
            "Decoder output (after self-attention, cross-attention, and feed-forward):\n",
            "tensor([[ 0.7727,  2.0374,  0.6013,  0.8936, -0.6175,  0.4774],\n",
            "        [ 0.0142,  0.0789,  0.7944,  1.5585, -0.4808, -0.0209]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Decoder Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_decoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4yiTNn0BkA"
      },
      "source": [
        "### Decoder Stack\n",
        "\n",
        "We'll use the same number of Decoder Blocks as we did Encoder Blocks - leaving us with 6 Deocder Blocks in our Decoder Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QUXzOXk0CcT"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, input_mask, target_mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRFiAP580S4b"
      },
      "source": [
        "## Linear Projection Layer\n",
        "\n",
        "After the decoder's self-attention and encoder-decoder attention layers, we have a context vector representing each Italian word. This context vector has a high dimension (e.g. 512 or 1024).\n",
        "\n",
        "We want to take this context vector and generate a probability distribution over the French vocabulary so we can pick the next translated word.\n",
        "\n",
        "The linear projection layer helps with this. It projects the context vector into a much larger vector called the vocabulary distribution - one entry per word in the vocabulary.\n",
        "\n",
        "For example, if our Italian vocabulary has 50,000 words, the vocabulary distribution will have 50,000 dimensions. Each dimension corresponds to the probability of that Italian word being the correct translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkBBMAZK0WLB"
      },
      "outputs": [],
      "source": [
        "class LinearProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return self.proj(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ucsRWs0lG9"
      },
      "source": [
        "## The Transformer\n",
        "\n",
        "At this point, all we need to do is create a class that represents our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ip11mmQ0nMM"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: EncoderBlock, decoder: DecoderBlock, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: LinearProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ssr5nA039--"
      },
      "source": [
        "## Building Our Transformer\n",
        "\n",
        "Now that we have each of our components - we need to construct an actual model!\n",
        "\n",
        "We'll use this helper function to aid in our goal and set up our Encoder/Decoder Stacks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdCt3wNi4EvY"
      },
      "outputs": [],
      "source": [
        "def build_transformer(input_vocab_size: int, target_vocab_size: int, input_seq_len: int, target_seq_len: int, d_model: int=512, N: int=6, num_heads: int=8, dropout: float=0.1, d_ff: int=2048, verbose=True) -> Transformer:\n",
        "  input_embeddings = InputEmbeddings(d_model, input_vocab_size, verbose=verbose)\n",
        "  target_embeddings = InputEmbeddings(d_model, target_vocab_size)\n",
        "\n",
        "  input_position = PositionalEncoding(d_model, input_seq_len, dropout, verbose=verbose)\n",
        "  target_position = PositionalEncoding(d_model, target_seq_len, dropout)\n",
        "\n",
        "  encoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  decoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    decoder_cross_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder_stack = EncoderStack(d_model, nn.ModuleList(encoder_blocks))\n",
        "  decoder_stack = DecoderStack(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  linear_projection_layer = LinearProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "  transformer = Transformer(encoder_stack, decoder_stack, input_embeddings, target_embeddings, input_position, target_position, linear_projection_layer)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2jfvmRx4ZNd"
      },
      "source": [
        "# Training Our Transformer!\n",
        "\n",
        "We will be using the resources created in [this](https://github.com/hkproj/pytorch-transformer/tree/main) repository to train our model on a English -> French translation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxF6JLcg5LJp"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "The BilingualDataset is a custom PyTorch dataset for working with translation data. It needs a tokenizer for each language, a dataset of sentence pairs, info on which languages are source and target, and the max sequence length.\n",
        "\n",
        "This class handles tokenizing the sentences, padding them to be the same length, and getting the data into the right format for sequence-to-sequence models. It adds special start, end, and padding tokens so all the inputs and outputs are the same length.\n",
        "\n",
        "When you grab a sample from the dataset, it tokenizes the source and target sentences, pads them, and creates the input tensors the model needs - encoder input, decoder input, and target labels. It also makes masks to show what's real data vs padding, and to make sure the decoder predictions only use previous tokens, not future ones.\n",
        "\n",
        "The BilingualDataset gets the data ready for training seq2seq models in a way that works with the sequential nature of language. The model can only predict the next token based on what came before it, not after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLnUZRgk7S-G"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "  def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "    super().__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "    self.ds = ds\n",
        "    self.tokenizer_src = tokenizer_src\n",
        "    self.tokenizer_tgt = tokenizer_tgt\n",
        "    self.src_lang = src_lang\n",
        "    self.tgt_lang = tgt_lang\n",
        "\n",
        "    self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "    self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "    self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src_target_pair = self.ds[idx]\n",
        "    src_text = src_target_pair['translation'][self.src_lang]\n",
        "    tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "    enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "    dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "    enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "    dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "    if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "        raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "    encoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    decoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    label = torch.cat(\n",
        "        [\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    assert encoder_input.size(0) == self.seq_len\n",
        "    assert decoder_input.size(0) == self.seq_len\n",
        "    assert label.size(0) == self.seq_len\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,\n",
        "        \"decoder_input\": decoder_input,\n",
        "        \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "        \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "        \"label\": label,\n",
        "        \"src_text\": src_text,\n",
        "        \"tgt_text\": tgt_text,\n",
        "    }\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "  return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hSYI7F8IAI"
      },
      "source": [
        "## Build Tokenizer For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfmUwnpo8qou",
        "outputId": "a9c7aaaf-2a26-4729-c0ee-4e4407508b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers tokenizers datasets -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCDOGq0UD1W"
      },
      "source": [
        "This will grab all the sentences from our dataset per language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAUUEXFz8eGH"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUDKOCTVUIa7"
      },
      "source": [
        "We'll quickly train a tokenizer on our dataset for both our source and target languages.\n",
        "\n",
        "We'll be sure to add the `[UNK]`, `[PAD]`, `[SOS]`, and `[EOS]` special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s1ZHzKb8hTN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def build_tokenizer(config, ds, lang):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "  tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEUDihkcVRq1"
      },
      "source": [
        "Now we can create our dataset in a format that our model expects and can train with!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI7xXnLo84cE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_ds(config):\n",
        "  # It only has the train split, so we divide it overselves\n",
        "  ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "  # Build tokenizers\n",
        "  tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "  tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "  # Keep 90% for training, 10% for validation\n",
        "  train_ds_size = int(0.9 * len(ds_raw))\n",
        "  val_ds_size = len(ds_raw) - train_ds_size\n",
        "  train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "  train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "  val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "  # Find the maximum length of each sentence in the source and target sentence\n",
        "  max_len_src = 0\n",
        "  max_len_tgt = 0\n",
        "\n",
        "  for item in ds_raw:\n",
        "    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "    max_len_src = max(max_len_src, len(src_ids))\n",
        "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "  print(f'Max length of source sentence: {max_len_src}')\n",
        "  print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "  train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "  val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr3Ys-ufVW1E"
      },
      "source": [
        "We can build our model with this helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK6k5X829JOo"
      },
      "outputs": [],
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "  model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'], verbose=False)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCALGXpf9tnv"
      },
      "outputs": [],
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "  return str(Path('.') / model_folder / model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPWZgCwD9vVX"
      },
      "outputs": [],
      "source": [
        "def latest_weights_file_path(config):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}*\"\n",
        "  weights_files = list(Path(model_folder).glob(model_filename))\n",
        "  if len(weights_files) == 0:\n",
        "      return None\n",
        "  weights_files.sort()\n",
        "  return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNjAS-LvV557"
      },
      "source": [
        "Finally....our training loop!\n",
        "\n",
        "We'll spend more time in following weeks discussing this - for now, we'll quickly walk through what's happening:\n",
        "\n",
        "1. Configure the training device (GPU/CPU) and print details. Set device in PyTorch.\n",
        "\n",
        "2. Create directory for saving model weights based on config.\n",
        "\n",
        "3. Get data loaders, tokenizers, and model. Move model to configured device.\n",
        "\n",
        "4. Initialize Adam optimizer with learning rate and epsilon from config.\n",
        "\n",
        "5. Set up initial training parameters like start epoch and global step.\n",
        "\n",
        "6. Define cross-entropy loss function with label smoothing, ignoring padding.\n",
        "\n",
        "---\n",
        "\n",
        "- Main training loop over epochs:\n",
        "\n",
        "  - Clear cache, set model to train mode, initialize progress bar.\n",
        "\n",
        "  - For each batch:\n",
        "\n",
        "    - Move data to device, run model forward/backward passes.\n",
        "    - Compute loss, backprop, update model weights.\n",
        "    - Increment global step.\n",
        "  - After each epoch, save model and optimizer checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL1bH7is9OfS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def train_model(config):\n",
        "  # Define the device\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "  print(\"Using device:\", device)\n",
        "  if (device == 'cuda'):\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "  else:\n",
        "    print(\"Please ensure you're in a GPU enabled Colab Notebook instance.\")\n",
        "  device = torch.device(device)\n",
        "\n",
        "  # Make sure the weights folder exists\n",
        "  Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "  model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "  initial_epoch = 0\n",
        "  global_step = 0\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "  for epoch in range(initial_epoch, config['num_epochs']):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "    for batch in batch_iterator:\n",
        "      encoder_input = batch['encoder_input'].to(device)\n",
        "      decoder_input = batch['decoder_input'].to(device)\n",
        "      encoder_mask = batch['encoder_mask'].to(device)\n",
        "      decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "      encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "      decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "      proj_output = model.project(decoder_output)\n",
        "\n",
        "      label = batch['label'].to(device)\n",
        "\n",
        "      loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      global_step += 1\n",
        "\n",
        "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "    torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'global_step': global_step\n",
        "    }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f21pxGoS9-gM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"batch_size\": 64,\n",
        "  \"num_epochs\": 6,\n",
        "  \"lr\": 1e-4,\n",
        "  \"seq_len\": 350,\n",
        "  \"d_model\": 512,\n",
        "  \"datasource\": 'opus_books',\n",
        "  \"lang_src\": \"en\",\n",
        "  \"lang_tgt\": \"it\",\n",
        "  \"model_folder\": \"trained__en_it_translation_model\",\n",
        "  \"model_basename\": \"encoder_decoder_model_\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfBcSHUo-MU6",
        "outputId": "dec1db65-a12c-42b1-dc34-394eab453b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "Device memory: 39.56427001953125 GB\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 00: 100%|██████████| 455/455 [05:26<00:00,  1.39it/s, loss=6.288]\n",
            "Processing Epoch 01: 100%|██████████| 455/455 [05:27<00:00,  1.39it/s, loss=5.811]\n",
            "Processing Epoch 02: 100%|██████████| 455/455 [05:27<00:00,  1.39it/s, loss=5.545]\n",
            "Processing Epoch 03: 100%|██████████| 455/455 [05:27<00:00,  1.39it/s, loss=5.524]\n",
            "Processing Epoch 04: 100%|██████████| 455/455 [05:27<00:00,  1.39it/s, loss=5.216]\n",
            "Processing Epoch 05: 100%|██████████| 455/455 [05:27<00:00,  1.39it/s, loss=5.005]\n"
          ]
        }
      ],
      "source": [
        "train_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT_QNpmqzAxe",
        "outputId": "7f679622-22bc-43b7-b848-8ce175635269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n",
            "Loading weights from opus_books_trained__en_it_translation_model/encoder_decoder_model_05.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-51-752505bf774c>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_filename)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): EncoderStack(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layernorm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): DecoderStack(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layernorm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(15698, 512)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(22463, 512)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): LinearProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=22463, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_model(config):\n",
        "    # Get dataloaders and tokenizers\n",
        "    _, _, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    # Initialize model\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n",
        "\n",
        "    # Load trained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    if model_filename:\n",
        "        print(f\"Loading weights from {model_filename}\")\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    return model, tokenizer_src, tokenizer_tgt, device\n",
        "\n",
        "def generate(model, tokenizer_src, tokenizer_tgt, src_text, device, max_length=350):\n",
        "    model.eval()\n",
        "\n",
        "    enc_input = tokenizer_src.encode(src_text).ids\n",
        "    enc_input = torch.tensor([tokenizer_src.token_to_id('[SOS]')] + enc_input + [tokenizer_src.token_to_id('[EOS]')]).unsqueeze(0)\n",
        "\n",
        "    enc_mask = (enc_input != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "    enc_input = enc_input.to(device)\n",
        "    enc_mask = enc_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encode(enc_input, enc_mask)\n",
        "        dec_input = torch.tensor([[tokenizer_tgt.token_to_id('[SOS]')]]).to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            dec_mask = causal_mask(dec_input.size(1)).to(device)\n",
        "\n",
        "            dec_output = model.decode(enc_output, enc_mask, dec_input, dec_mask)\n",
        "            proj_output = model.project(dec_output)\n",
        "\n",
        "            next_word = proj_output[:, -1].argmax(dim=-1)\n",
        "            dec_input = torch.cat([dec_input, next_word.unsqueeze(-1)], dim=1)\n",
        "\n",
        "            if next_word.item() == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    translated_tokens = [tokenizer_tgt.id_to_token(t.item()) for t in dec_input[0]]\n",
        "    translated_text = ' '.join([t for t in translated_tokens if t not in ['[SOS]', '[EOS]', '[PAD]']])\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "model, tokenizer_src, tokenizer_tgt, device = load_model(config)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B1hXBPhsd4p",
        "outputId": "267033cd-46c2-48bd-f172-f5c8b397d913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English to Italian Translations:\n",
            "--------------------------------------------------\n",
            "EN: the weather is beautiful today\n",
            "IT: Il giorno è stato , ma è stato bello .\n",
            "--------------------------------------------------\n",
            "EN: how are you?\n",
            "IT: Come siete ?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [\n",
        "        \"the weather is beautiful today\",\n",
        "        \"how are you?\"\n",
        "    ]\n",
        "\n",
        "print(\"English to Italian Translations:\")\n",
        "print(\"-\" * 50)\n",
        "for sentence in test_sentences:\n",
        "    translation = generate(model, tokenizer_src, tokenizer_tgt, sentence, device)\n",
        "    print(f\"EN: {sentence}\")\n",
        "    print(f\"IT: {translation}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Cofqp53bQB"
      },
      "source": [
        "#### Acknowledgements\n",
        "\n",
        "This notebook is heavily adapted from a number of incredible resources on Transformers, including but not limited to:\n",
        "\n",
        "- https://blog.floydhub.com/the-transformer-in-pytorch/\n",
        "- https://arxiv.org/pdf/1706.03762.pdf\n",
        "- https://txt.cohere.com/what-are-transformer-models/\n",
        "- https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}